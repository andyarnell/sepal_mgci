{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andyarnell/sepal_mgci/blob/master/SDG_15_4_2_Sub_A_Default_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SDG 15.4.2 Subcomponent A: Calculate Global Default Values**\n",
        "\n",
        "* This script allows batch processing for this indicator for all countries.\n",
        "\n",
        "* Output is a combined excel file on your Google Drive.\n",
        "\n",
        "* Runs on the cloud using [Google Colab](https://research.google.com/colaboratory/faq.html)\n",
        "\n",
        "* Requires: [Google Earth Engine](https://earthengine.google.com/) (GEE) account and project and access to Google Drive\n"
      ],
      "metadata": {
        "id": "82FWR5yMh0HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required packages\n",
        "NB may get error requiring a restart on first run only (works after this - just rerun from start)\n",
        "\n",
        "TO DO Check with Daniel if can shift sepal_ui import requirement"
      ],
      "metadata": {
        "id": "YdoCOI_1yWY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "Kg3K1EPJu_f5",
        "outputId": "b807bcad-6fd5-4f4e-d555-0f287966f379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ee is already installed.\n",
            "geemap is already installed.\n",
            "unidecode is already installed.\n",
            "google-api-python-client has been installed.\n",
            "google-auth-httplib2 has been installed.\n",
            "google-auth-oauthlib has been installed.\n",
            "sepal_ui is already installed.\n"
          ]
        }
      ],
      "source": [
        "# to automatically reload modules.\n",
        "%load_ext autoreload\n",
        "\n",
        "# Set to reload all modules before executing code.\n",
        "%autoreload 2\n",
        "\n",
        "# Function to install a package if it's not already installed\n",
        "def install_if_not_exists(package_name):\n",
        "    try:\n",
        "        __import__(package_name)\n",
        "        print(f\"{package_name} is already installed.\")\n",
        "    except ImportError:\n",
        "        !pip install -q {package_name}\n",
        "        print(f\"{package_name} has been installed.\")\n",
        "\n",
        "# List of packages to install if not already installed\n",
        "packages_to_install = ['ee', 'geemap', 'unidecode', 'google-api-python-client',\n",
        "                      'google-auth-httplib2', 'google-auth-oauthlib','sepal_ui']\n",
        "\n",
        "# Install necessary packages\n",
        "for package in packages_to_install:\n",
        "    install_if_not_exists(package)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages"
      ],
      "metadata": {
        "id": "L5jsWgqWyiCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import ee # google earth engine\n",
        "\n",
        "ee.Authenticate()\n",
        "\n",
        "ee.Initialize(project=\"ee-andyarnellgee\") # NB gee project name is defined in parameters section\n",
        "\n",
        "from datetime import datetime # for time stamping error log\n",
        "import pandas as pd # pandas library for tabular data manipulation\n",
        "import re # for manipulating strings\n",
        "from unidecode import unidecode # converting symbols in country names to ascii compliant (required for naming GEE tasks)\n",
        "\n",
        "# formatting excel report file\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "# Change current directory to sepal_mgci (i.e. the local copy of the github repository)\n",
        "%cd \"/content/sepal_mgci\"\n",
        "\n",
        "# Import parameters for the default DEM asset and a lookup table for land cover reclassification\n",
        "from component.parameter.module_parameter import DEM_DEFAULT, LC_MAP_MATRIX\n",
        "\n",
        "# # # # Import scripts and modules from cloned GitHub repository (i.e., functions for indicator calculation and formatting)\n",
        "from component.scripts.gee import reduce_regions # for running summary statistics in GEE\n",
        "from component.scripts.scripts import get_a_years, map_matrix_to_dict, parse_result, read_from_csv# parameter prep and reformatting\n",
        "from component.scripts import sub_a, sub_b, mountain_area as mntn ###TO DO: ADD DESCRIPTIONS\n",
        "\n",
        "# print(\"Imports complete\")\n"
      ],
      "metadata": {
        "id": "xopY6C80GXl6",
        "outputId": "8181a430-88fc-4837-ea37-f1666d4d133c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sepal_mgci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set parameters and install packages\n"
      ],
      "metadata": {
        "id": "Bi0XpRRDo6V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs"
      ],
      "metadata": {
        "id": "5aIr_OxkG-Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Earth Engine project\n",
        "gee_project_name = \"insert cloud project here\"  # a registered cloud project (if unsure of name see pic here: https://developers.google.com/earth-engine/cloud/assets)\n",
        "\n",
        "\n",
        "# Admin boundaries asset\n",
        "admin_asset_id = \"FAO/GAUL/2015/level0\" # administrative units feature collection\n",
        "\n",
        "admin_asset_property_name = \"ADM0_NAME\" # property/column name for selecting admin boundaries (e.g. ISO3 code or country name)\n",
        "\n",
        "\n",
        "# Land cover assets\n",
        "\n",
        "# For SUB_A indicator, we need to set the following structure.\n",
        "a_years = {\n",
        "    1: {\"asset\": \"users/amitghosh/sdg_module/esa/cci_landcover/2000\", \"year\": 2000}, # baseline\n",
        "    2: {\"year\": 2003, \"asset\": \"users/amitghosh/sdg_module/esa/cci_landcover/2003\"}, # subsequent reporting years...\n",
        "    3: {\"year\": 2007, \"asset\": \"users/amitghosh/sdg_module/esa/cci_landcover/2007\"},\n",
        "    4: {\"year\": 2010, \"asset\": \"users/amitghosh/sdg_module/esa/cci_landcover/2010\"},\n",
        "}\n"
      ],
      "metadata": {
        "id": "t9C1qT5bGoqt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "naz3Qe4JHFER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_report_folder = \"sdg_15_4_2_A_combined_report\" # folder name in Google Drive for final output (if doesnt exist creates one)\n",
        "\n",
        "final_report_name = \"sdg_15_4_2_A_default_global.xlsx\" # file name for final excel output\n",
        "\n",
        "# export GEE tasks or not\n",
        "export = False # default: True. Set to False if debugging or limiting accidental re-exporting of tasks\n",
        "\n",
        "# prints more messages\n",
        "debug = False # default: False. Set to True if debugging code"
      ],
      "metadata": {
        "id": "mSvTM29TnndR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporary outputs\n"
      ],
      "metadata": {
        "id": "iV8-wbddOJuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats_csv_folder = \"sdg_15_4_2_A_csvs\" # for storing stats tables exported from GEE for each admin boundary/AOI\n",
        "\n",
        "excel_reports_folder = \"sdg_15_4_2_A_reports\" # for storing formatted excel tables for each admin boundary/AOI\n",
        "\n",
        "drive_home =\"/content/drive/MyDrive/\" # Google Drive location. Don't change unless you know this is incorrect\n",
        "\n",
        "error_log_file_path = drive_home + excel_reports_folder + \"/\"+\"1_error_log\" +\".csv\" # for storing errors\n"
      ],
      "metadata": {
        "id": "TCBNkALuOL1N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access GitHub repository\n",
        "Clones repository for SDG 15.4.2 into colab.\n",
        "Provides functions and lookup tables etc."
      ],
      "metadata": {
        "id": "qGfLFLEkwS1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the current working directory to \"/content\".\n",
        "%cd \"/content\"\n",
        "\n",
        "# Clone the GitHub repository \"sepal_mgci\" into the current directory.\n",
        "# NB 'fatal' error on reruns are typically just saying it already exists\n",
        "!git clone https://github.com/sepal-contrib/sepal_mgci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNohZrOTvNqT",
        "outputId": "3f3bedfc-98f5-45af-d17e-538abfe50d05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'sepal_mgci'...\n",
            "remote: Enumerating objects: 2897, done.\u001b[K\n",
            "remote: Counting objects: 100% (835/835), done.\u001b[K\n",
            "remote: Compressing objects: 100% (296/296), done.\u001b[K\n",
            "remote: Total 2897 (delta 552), reused 729 (delta 538), pack-reused 2062\u001b[K\n",
            "Receiving objects: 100% (2897/2897), 4.89 MiB | 10.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1828/1828), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Google Earth Engine\n",
        "Launches access request pop up window"
      ],
      "metadata": {
        "id": "FAzzmzMHzFmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ee # google earth engine\n",
        "\n",
        "# ee.Authenticate()\n",
        "\n",
        "# ee.Initialize(project=\"ee-andyarnellgee\") # NB gee project name is defined in parameters section"
      ],
      "metadata": {
        "id": "gHI-M8y_qwaP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Google Drive\n",
        "Launches access request pop up window"
      ],
      "metadata": {
        "id": "aUVGL6zwLpX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for accessing google drive\n",
        "from google.colab import auth, drive\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NorUzAJ8Kj0Z",
        "outputId": "97986cd8-be48-4285-f575-d3856997e69f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions\n",
        "TO DO: these will be stored somewhere else in GitHub hopefully - if Daniel is happy"
      ],
      "metadata": {
        "id": "ShGLbAVpL_C_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IA0zW-cIu_f8"
      },
      "outputs": [],
      "source": [
        "def folder_exists(folder_name, parent_folder_id=None):\n",
        "    \"\"\"\n",
        "    Check if a folder exists in Google Drive.\n",
        "\n",
        "    Args:\n",
        "    - folder_name (str): Name of the folder to check.\n",
        "    - parent_folder_id (str): ID of the parent folder where to search for the folder.\n",
        "                              Default is None, meaning the search will be performed in the root.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the folder exists, False otherwise.\n",
        "    \"\"\"\n",
        "    # Authenticate user\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Build the Drive v3 service\n",
        "    drive_service = build('drive', 'v3')\n",
        "\n",
        "    # Prepare query to check if folder exists\n",
        "    query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "    if parent_folder_id:\n",
        "        query += f\" and '{parent_folder_id}' in parents\"\n",
        "\n",
        "    try:\n",
        "        # Execute the search query\n",
        "        folders = drive_service.files().list(q=query, fields='files(id)', includeItemsFromAllDrives=True, supportsAllDrives=True).execute().get('files', [])\n",
        "        return bool(folders)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_folder(folder_name, parent_folder_id=None):\n",
        "    \"\"\"\n",
        "    Create a folder in Google Drive.\n",
        "\n",
        "    Args:\n",
        "    - folder_name (str): Name of the folder to be created.\n",
        "    - parent_folder_id (str): ID of the parent folder where the new folder will be created.\n",
        "                              Default is None, meaning the folder will be created in the root.\n",
        "\n",
        "    Returns:\n",
        "    - str: ID of the newly created folder.\n",
        "    \"\"\"\n",
        "    # Authenticate user\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Build the Drive v3 service\n",
        "    drive_service = build('drive', 'v3')\n",
        "\n",
        "    # Prepare folder metadata\n",
        "    folder_metadata = {\n",
        "        'name': folder_name,\n",
        "        'mimeType': 'application/vnd.google-apps.folder'\n",
        "    }\n",
        "    if parent_folder_id:\n",
        "        folder_metadata['parents'] = [parent_folder_id]\n",
        "\n",
        "    # Create the folder\n",
        "    folder = drive_service.files().create(body=folder_metadata, fields='id').execute()\n",
        "\n",
        "    # Return the ID of the newly created folder\n",
        "    return folder.get('id')\n",
        "\n",
        "\n",
        "def create_folder_if_not_exists(folder_name, parent_folder_id=None):\n",
        "    \"\"\"\n",
        "    Create a folder in Google Drive if it doesn't already exist.\n",
        "\n",
        "    Args:\n",
        "    - folder_name (str): Name of the folder to be created.\n",
        "    - parent_folder_id (str): ID of the parent folder where the new folder will be created.\n",
        "                              Default is None, meaning the folder will be created in the root.\n",
        "\n",
        "    Returns:\n",
        "    - str: ID of the newly created folder or the existing folder if it already exists.\n",
        "    \"\"\"\n",
        "    if folder_exists(folder_name, parent_folder_id):\n",
        "        print(f\"Folder '{folder_name}' already exists.\")\n",
        "        return None\n",
        "    else:\n",
        "        return create_folder(folder_name, parent_folder_id)\n",
        "\n",
        "\n",
        "def sanitize_description(description):\n",
        "    allowed_characters_pattern = r\"[^a-zA-Z0-9.,:;_ \\-]\"  # Define a regex pattern for characters not in the allowed set\n",
        "    sanitized_description = re.sub(allowed_characters_pattern, \"\", description)  # Remove characters not in the allowed set\n",
        "    return sanitized_description\n",
        "\n",
        "\n",
        "def append_excel_files(file_paths, num_sheets, output_file_path):\n",
        "    # Initialize a dictionary to store combined DataFrames from different files\n",
        "    combined_dfs = {}\n",
        "\n",
        "    # Initialize a counter to track the progress of file processing\n",
        "    counter = 0\n",
        "\n",
        "    # Iterate over each file path in the list\n",
        "    for file_path in file_paths:\n",
        "        # Load the Excel file\n",
        "        # xls = pd.ExcelFile(file_path)  # Reads file and stores as an ExcelFile object (using the Pandas library)\n",
        "        xls = pd.ExcelFile(file_path, engine='openpyxl')  # Reads file and stores as an ExcelFile object (using the Pandas library)\n",
        "\n",
        "        # xls = pd.ExcelFile(file_path, engine='xlrd')  # Reads file and stores as an ExcelFile object (using the Pandas library)\n",
        "\n",
        "        # Increment the counter for each iteration\n",
        "        counter += 1\n",
        "\n",
        "        # Read each sheet from the Excel file into a DataFrame\n",
        "        # Only read up to num_sheets specified\n",
        "        dfs = {sheet_name: xls.parse(sheet_name) for sheet_name in xls.sheet_names[:num_sheets]}\n",
        "\n",
        "        # Append the DataFrames to the combined_dfs dictionary\n",
        "        for sheet_name, df in dfs.items():\n",
        "            if sheet_name in combined_dfs:\n",
        "                # If the sheet already exists in combined_dfs, concatenate the current DataFrame with the existing one\n",
        "                combined_dfs[sheet_name] = pd.concat([combined_dfs[sheet_name], df], ignore_index=True)\n",
        "            else:\n",
        "                # If the sheet does not exist in combined_dfs, add the DataFrame directly\n",
        "                combined_dfs[sheet_name] = df\n",
        "\n",
        "        # Print the progress of processing, overwriting the previous progress\n",
        "        print(f\"\\rProcessing {counter}/{len(file_paths)}: {file_path}\", end=\"\")\n",
        "\n",
        "    # Write the combined DataFrames to the specified output file path\n",
        "    with pd.ExcelWriter(output_file_path) as writer:\n",
        "        for sheet_name, df in combined_dfs.items():\n",
        "            # Write each DataFrame to a separate sheet in the output Excel file\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_8UzlUnu_f6"
      },
      "source": [
        "## SUB INDICATOR A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create list of boundaries to process"
      ],
      "metadata": {
        "id": "qi5zNGTWN3df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# admin boundary\n",
        "admin_boundaries = ee.FeatureCollection(admin_asset_id)\n",
        "\n",
        "# list to process\n",
        "list_of_countries = admin_boundaries.aggregate_array(admin_asset_property_name).getInfo()\n",
        "\n",
        "print (\"Length of admin boundaries to process\", len(list_of_countries))\n",
        "\n",
        "list_of_countries = list(set(list_of_countries)) # remove dupicates\n",
        "\n",
        "print (\"Length of distinct admin boundaries to process\", (len(set(list_of_countries))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "BiuBEJwPue2v",
        "outputId": "d8342cf0-1266-42c1-e08d-8e23aaee4e6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'admin_asset_id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# admin boundary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m admin_boundaries \u001b[38;5;241m=\u001b[39m ee\u001b[38;5;241m.\u001b[39mFeatureCollection(\u001b[43madmin_asset_id\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# list to process\u001b[39;00m\n\u001b[1;32m      5\u001b[0m list_of_countries \u001b[38;5;241m=\u001b[39m admin_boundaries\u001b[38;5;241m.\u001b[39maggregate_array(admin_asset_property_name)\u001b[38;5;241m.\u001b[39mgetInfo()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'admin_asset_id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the default land cover remapping table and convert it to a dictionary"
      ],
      "metadata": {
        "id": "2G1q9TSiUsc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRSEqq5bu_f7"
      },
      "outputs": [],
      "source": [
        "default_map_matrix = map_matrix_to_dict(LC_MAP_MATRIX)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select years of land cover to process"
      ],
      "metadata": {
        "id": "4-Ut_-S35Yg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwqJFWR4u_f7"
      },
      "outputs": [],
      "source": [
        "# extracts the years from the a_years dictionary (as defined in parameters)\n",
        "single_years = [y[\"year\"] for  y in a_years.values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run area statistics within admin boundaries\n",
        "* Runs for each country and each mountain biobelt\n",
        "* Gets area of land cover reclassified into the 10 SEAM classes\n",
        "* Repeat for each year specified\n"
      ],
      "metadata": {
        "id": "iNxHtR984cNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VftKLuY4u_f7"
      },
      "outputs": [],
      "source": [
        "# you can monitor your GEE tasks here : https://code.earthengine.google.com/tasks\n",
        "\n",
        "create_folder_if_not_exists(stats_csv_folder) # to store outputs in google drive\n",
        "\n",
        "counter=0 # starting place of counter used to keep track of number of tasks that are being run\n",
        "\n",
        "for aoi_name in list_of_countries:\n",
        "\n",
        "    aoi = admin_boundaries.filter(ee.Filter.eq(admin_asset_property_name,aoi_name))#.first()\n",
        "\n",
        "    # gets areas of landcover in each mountain belt in each country\n",
        "    # uses reduce_regions function imported from the cloned sepal_mgci git hub repository (see Imports section)\n",
        "    # pixels counted at native resolution (scale) of input land cover (or DEM if RSA implementation)\n",
        "    process = ee.FeatureCollection([\n",
        "        ee.Feature(\n",
        "            None,\n",
        "            reduce_regions(\n",
        "                aoi,\n",
        "                remap_matrix=default_map_matrix,\n",
        "                rsa=False,\n",
        "                # dem=param.DEM_DEFAULT,\n",
        "                dem=DEM_DEFAULT, #default digital elevation model (DEM). Relevant for the real surface area (RSA) implementation.\n",
        "                lc_years= year,\n",
        "                transition_matrix=False\n",
        "            )\n",
        "        ).set(\"process_id\", year[0][\"year\"])\n",
        "        for year in get_a_years(a_years) # creates GEE images and runs stats on each. Images to run are in the 'a_years\" dictionary (above)\n",
        "    ])\n",
        "\n",
        "    #make name acceptable for running tasks (i.e., removes special characters)\n",
        "    task_name = str(sanitize_description(unidecode(aoi_name)))\n",
        "\n",
        "\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        **{  #asterisks unpack dictionary into keyword arguments format\n",
        "            \"collection\": process,\n",
        "            \"description\": task_name,\n",
        "            \"fileFormat\": \"CSV\",\n",
        "            \"folder\":stats_csv_folder,\n",
        "            \"selectors\": [\n",
        "                \"process_id\",\n",
        "                \"sub_a\",\n",
        "            ],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    counter+=1\n",
        "\n",
        "    print (f\"\\r process {counter}/{len(list_of_countries)} {aoi_name} \", end=\"\") #print in place (remove \\r and end=\"\" for verbose version)\n",
        "\n",
        "    if export:\n",
        "      task.start()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM0SIvJtu_f8"
      },
      "source": [
        "# Read, process, and create report tables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Manually check your earth engine task status, once the tasks are complete, run the next cell. https://code.earthengine.google.com/tasks\n",
        "\n",
        "This formats individual excel reports for each country.\n",
        "See Error_log.csv for missing files/errors"
      ],
      "metadata": {
        "id": "3x7jwkZJWwE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkpDfHqQu_f9"
      },
      "outputs": [],
      "source": [
        "# Initialize the counter\n",
        "counter = 0\n",
        "\n",
        "# to store outputs in google drive\n",
        "create_folder_if_not_exists(excel_reports_folder)\n",
        "\n",
        "# Loop over each AOI name in the list of countries\n",
        "for aoi_name in list_of_countries:\n",
        "    counter += 1\n",
        "\n",
        "    # Clean the AOI name\n",
        "    aoi_name_clean = str(sanitize_description(unidecode(aoi_name)))\n",
        "\n",
        "    # Construct the file path for the stats CSV file\n",
        "    stats_csv_file = aoi_name_clean + \".csv\"\n",
        "    stats_csv_file_path = os.path.join(drive_home, stats_csv_folder, stats_csv_file)\n",
        "\n",
        "    message = f\"Process {counter}, {stats_csv_file}\"\n",
        "\n",
        "    try:\n",
        "        # Read the results from the CSV file and parse it to a dictionary\n",
        "        dict_results = read_from_csv(stats_csv_file_path)\n",
        "\n",
        "        details = {\n",
        "            \"geo_area_name\": aoi_name,\n",
        "            \"ref_area\": \" \",\n",
        "            \"source_detail\": \" \",\n",
        "        }\n",
        "\n",
        "        # Generate reports for the sub_a and mtn indicators\n",
        "        sub_a_reports = [sub_a.get_reports(parse_result(dict_results[year][\"sub_a\"], single=True), year, **details) for year in single_years]\n",
        "        mtn_reports = [mntn.get_report(parse_result(dict_results[year][\"sub_a\"], single=True), year, **details) for year in single_years]\n",
        "\n",
        "        # Concatenate the mtn reports\n",
        "        mtn_reports_df = pd.concat(mtn_reports)\n",
        "\n",
        "        # Concatenate the sub a reports\n",
        "        er_mtn_grnvi_df = pd.concat([report[0] for report in sub_a_reports])\n",
        "        er_mtn_grncov_df = pd.concat([report[1] for report in sub_a_reports])\n",
        "\n",
        "        # Define the output report file path\n",
        "        report_file_path = os.path.join(drive_home, excel_reports_folder, aoi_name_clean + \".xlsx\")\n",
        "\n",
        "        # Create the Excel file with the reports\n",
        "        with pd.ExcelWriter(report_file_path) as writer:\n",
        "            mtn_reports_df.to_excel(writer, sheet_name=\"Table1_ER_MTN_TOTL\", index=False)\n",
        "            er_mtn_grncov_df.to_excel(writer, sheet_name=\"Table2_ER_MTN_GRNCOV\", index=False)\n",
        "            er_mtn_grnvi_df.to_excel(writer, sheet_name=\"Table3_ER_MTN_GRNCVI\", index=False)\n",
        "\n",
        "            # Adjust column widths and alignment for each sheet\n",
        "            for sheetname in writer.sheets:\n",
        "                worksheet = writer.sheets[sheetname]\n",
        "                for col in worksheet.columns:\n",
        "                    max_length = max(len(str(cell.value)) for cell in col)\n",
        "                    column = col[0]\n",
        "                    adjusted_width = max(max_length, len(str(column.value))) + 4\n",
        "                    worksheet.column_dimensions[get_column_letter(column.column)].width = adjusted_width\n",
        "\n",
        "                    # Align \"obs_value\" column to the right\n",
        "                    if \"OBS\" in column.value:\n",
        "                        for cell in col:\n",
        "                            cell.alignment = Alignment(horizontal=\"right\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If an error occurs, catch the exception and handle it\n",
        "        message = f\"process {counter}, {stats_csv_file}, Error: {e}\"\n",
        "\n",
        "        # Get the current time\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Write the error message and file name to the error log file\n",
        "        error_info = pd.DataFrame([[stats_csv_file, str(e), current_time]], columns=['File Name', 'Error Message', 'Time'])\n",
        "\n",
        "        mode = 'w' if not os.path.exists(error_log_file_path) else 'a'\n",
        "        header = False if os.path.exists(error_log_file_path) else True\n",
        "\n",
        "        # Append or write to the error log file\n",
        "        error_info.to_csv(error_log_file_path, mode=mode, header=header, index=False)\n",
        "\n",
        "    print(message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combine excel files"
      ],
      "metadata": {
        "id": "qTqI3Ag08k5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a list of files to combine"
      ],
      "metadata": {
        "id": "sVpLtG4Z7Jbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path where Excel reports are stored\n",
        "directory_path = os.path.join(drive_home, excel_reports_folder)\n",
        "\n",
        "# List files in the directory with '.xlsx' extension\n",
        "files = [file for file in os.listdir(directory_path) if file.endswith('.xlsx')]\n",
        "\n",
        "# Create a list of full file paths\n",
        "full_file_paths = [os.path.join(directory_path, file) for file in files]\n",
        "\n",
        "# Print the number of Excel files found in the folder\n",
        "print(f\"Number of Excel files in folder: {len(full_file_paths)}\")\n",
        "\n",
        "# folder to store outputs in google drive\n",
        "create_folder_if_not_exists(final_report_folder)\n",
        "\n",
        "# File path for the combined final report\n",
        "reports_combined_file_path = os.path.join(drive_home, final_report_folder, final_report_name)\n"
      ],
      "metadata": {
        "id": "sTyhwne7rr9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Run function to combine into a single report"
      ],
      "metadata": {
        "id": "C77flAQu7xWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "append_excel_files(file_paths=full_file_paths,num_sheets=3,output_file_path=reports_combined_file_path)\n",
        "\n",
        "print (f\"\\n Complete! Output file for SDG 15.4.2 Component A here: {reports_combined_file_path}\")"
      ],
      "metadata": {
        "id": "FkS1ErA9AYj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKSnKCYOBelT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "(test) test-sepal_mgci",
      "language": "python",
      "name": "test-sepal_mgci"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}